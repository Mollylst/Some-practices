{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom_dataset_trainandtest.ipynb",
      "provenance": [],
      "mount_file_id": "1KQosg_YNDiMSm8pptHcSBgpafoIoUoqr",
      "authorship_tag": "ABX9TyNrPRGCvxmYwx+EryVJO11z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mollylst/resnet18/blob/main/custom_dataset_trainandtest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCosunpl4-2_"
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
        "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
        "import torchvision\n",
        "import os\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from torch.utils.data import (\n",
        "    Dataset,\n",
        "    DataLoader,\n",
        ")  # Gives easier dataset managment and creates mini batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t_FHB5N43Fg"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((132, 132)),  \n",
        "    transforms.RandomCrop((128, 128)),  \n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "     transforms.ToPILImage(),\n",
        "     transforms.Resize((132, 132)),  \n",
        "     transforms.RandomCrop((128, 128)), \n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "class ApplyTransform(Dataset):\n",
        "    \"\"\"\n",
        "    Apply transformations to a Dataset\n",
        "\n",
        "    Arguments:\n",
        "        dataset (Dataset): A Dataset that returns (sample, target)\n",
        "        transform (callable, optional): A function/transform to be applied on the sample\n",
        "        target_transform (callable, optional): A function/transform to be applied on the target\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, transform=None, target_transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        # yes, you don't need these 2 lines below :(\n",
        "        if transform is None and target_transform is None:\n",
        "            print(\"Am I a joke to you? :)\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample, target = self.dataset[idx]\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(sample)\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "        return sample, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLtlUV8evwCu",
        "outputId": "c026b1ce-2237-4efd-ff11-4c10c935205e"
      },
      "source": [
        "class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
        "        image = io.imread(img_path)\n",
        "        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return (image, y_label)\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# Hyperparameters\n",
        "in_channel = 3\n",
        "num_classes = 2\n",
        "batch_size = 32\n",
        "\n",
        "# Load Data\n",
        "dataset = DeepfakeDataset(\n",
        "    csv_file=\"/content/drive/MyDrive/files_and_labels.csv\",\n",
        "    root_dir=\"/content/drive/MyDrive/real_and_fake_face_together\",\n",
        "    transform=transforms.ToTensor(),\n",
        ")\n",
        "\n",
        "#split\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [1632, 409])\n",
        "#lengths = [int(len(dataset)*0.8), int(len(dataset)*0.2)]\n",
        "#lengths = [1632, 409]\n",
        "#subsetA, subsetB = torch.utils.data.random_split(dataset, lengths)\n",
        "\n",
        "trainset = ApplyTransform(train_set,transform=transform_train)\n",
        "testset = ApplyTransform(test_set,transform=transform_test)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_c7QeRJ7TBf"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.reshape(out.shape[0],-1)\n",
        "        #out = F.avg_pool2d(out, 4)\n",
        "        #out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "#define ResNet18\n",
        "net = ResNet18().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXIfvdpu7XVN"
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15, eta_min=0, last_epoch=-1, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LAwcHyPH7dDU",
        "outputId": "8f1bbc25-4a96-4584-f48d-abf8731bf50e"
      },
      "source": [
        "import time\n",
        "num_epoch=15\n",
        "\n",
        "def train_and_test():\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "    best_epoch = 0\n",
        "    \n",
        "    for epoch in range(0,num_epoch):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, num_epoch))\n",
        "\n",
        "        net.train()\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        test_loss = 0.0\n",
        "        test_acc = 0.0\n",
        "\n",
        "        #train\n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        " \n",
        "            #the gradient is increasing, so make it zero\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        " \n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "        #test\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = net(inputs)\n",
        " \n",
        "                loss = criterion(outputs, labels)\n",
        " \n",
        "                test_loss += loss.item() * inputs.size(0)\n",
        " \n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        " \n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        " \n",
        "                test_acc += acc.item() * inputs.size(0)\n",
        " \n",
        "        avg_train_loss = train_loss/ len(trainset)\n",
        "        avg_train_acc = train_acc/ len(trainset)\n",
        " \n",
        "        avg_test_loss = test_loss/len(testset)\n",
        "        avg_test_acc = test_acc/len(testset)\n",
        "\n",
        "        #record the accuracy and loss in each epoch\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        " \n",
        "        if best_acc < avg_test_acc:\n",
        "            best_acc = avg_test_acc\n",
        "            best_epoch = epoch + 1\n",
        "\n",
        "        scheduler.step()  \n",
        "\n",
        "        epoch_end = time.time()\n",
        "        \n",
        "        print(\"Epoch: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tTest: Loss: {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(\n",
        "            epoch+1, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start\n",
        "        ))\n",
        "        print(\"Best Accuracy for test : {:.4f} at epoch {:03d}\".format(best_acc, best_epoch))\n",
        "          \n",
        "    return history\n",
        "    \n",
        "#start training and testing\n",
        "history = train_and_test()\n",
        "\n",
        "#make the plots\n",
        "history = np.array(history)\n",
        "plt.plot(history[:, 0:2])\n",
        "plt.legend(['Training Loss', 'Test Loss'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0, 1.1)\n",
        "#plt.savefig(dataset+'_loss_curve.png')\n",
        "plt.show()\n",
        " \n",
        "plt.plot(history[:, 2:4])\n",
        "plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1.1)\n",
        "#plt.savefig(dataset+'_accuracy_curve.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/30\n",
            "Epoch: 001, Training: Loss: 0.9709, Accuracy: 48.0392%, \n",
            "\t\tTest: Loss: 0.7566, Accuracy: 46.4548%, Time: 335.2110s\n",
            "Best Accuracy for test : 0.4645 at epoch 001\n",
            "Epoch: 2/30\n",
            "Epoch: 002, Training: Loss: 0.8035, Accuracy: 51.2868%, \n",
            "\t\tTest: Loss: 0.9435, Accuracy: 46.4548%, Time: 158.4939s\n",
            "Best Accuracy for test : 0.4645 at epoch 001\n",
            "Epoch: 3/30\n",
            "Epoch: 003, Training: Loss: 0.7387, Accuracy: 55.8211%, \n",
            "\t\tTest: Loss: 0.7203, Accuracy: 53.0562%, Time: 156.7049s\n",
            "Best Accuracy for test : 0.5306 at epoch 003\n",
            "Epoch: 4/30\n",
            "Epoch: 004, Training: Loss: 0.6760, Accuracy: 59.5588%, \n",
            "\t\tTest: Loss: 0.7669, Accuracy: 50.6112%, Time: 158.9065s\n",
            "Best Accuracy for test : 0.5306 at epoch 003\n",
            "Epoch: 5/30\n",
            "Epoch: 005, Training: Loss: 0.6500, Accuracy: 60.9069%, \n",
            "\t\tTest: Loss: 0.6952, Accuracy: 56.7237%, Time: 157.8010s\n",
            "Best Accuracy for test : 0.5672 at epoch 005\n",
            "Epoch: 6/30\n",
            "Epoch: 006, Training: Loss: 0.6587, Accuracy: 60.9069%, \n",
            "\t\tTest: Loss: 0.7509, Accuracy: 53.3007%, Time: 159.1344s\n",
            "Best Accuracy for test : 0.5672 at epoch 005\n",
            "Epoch: 7/30\n",
            "Epoch: 007, Training: Loss: 0.6244, Accuracy: 64.7672%, \n",
            "\t\tTest: Loss: 0.7308, Accuracy: 53.7897%, Time: 159.3943s\n",
            "Best Accuracy for test : 0.5672 at epoch 005\n",
            "Epoch: 8/30\n",
            "Epoch: 008, Training: Loss: 0.6041, Accuracy: 66.9730%, \n",
            "\t\tTest: Loss: 0.8179, Accuracy: 50.1222%, Time: 158.1967s\n",
            "Best Accuracy for test : 0.5672 at epoch 005\n",
            "Epoch: 9/30\n",
            "Epoch: 009, Training: Loss: 0.6345, Accuracy: 66.2377%, \n",
            "\t\tTest: Loss: 0.7292, Accuracy: 56.9682%, Time: 157.4600s\n",
            "Best Accuracy for test : 0.5697 at epoch 009\n",
            "Epoch: 10/30\n",
            "Epoch: 010, Training: Loss: 0.6240, Accuracy: 65.3186%, \n",
            "\t\tTest: Loss: 0.9988, Accuracy: 52.0782%, Time: 158.9162s\n",
            "Best Accuracy for test : 0.5697 at epoch 009\n",
            "Epoch: 11/30\n",
            "Epoch: 011, Training: Loss: 0.6580, Accuracy: 64.8284%, \n",
            "\t\tTest: Loss: 0.9173, Accuracy: 51.1002%, Time: 157.5508s\n",
            "Best Accuracy for test : 0.5697 at epoch 009\n",
            "Epoch: 12/30\n",
            "Epoch: 012, Training: Loss: 0.6387, Accuracy: 67.2181%, \n",
            "\t\tTest: Loss: 0.7925, Accuracy: 58.4352%, Time: 158.6171s\n",
            "Best Accuracy for test : 0.5844 at epoch 012\n",
            "Epoch: 13/30\n",
            "Epoch: 013, Training: Loss: 0.6451, Accuracy: 68.2598%, \n",
            "\t\tTest: Loss: 0.8913, Accuracy: 58.1907%, Time: 157.5881s\n",
            "Best Accuracy for test : 0.5844 at epoch 012\n",
            "Epoch: 14/30\n",
            "Epoch: 014, Training: Loss: 0.5676, Accuracy: 72.1814%, \n",
            "\t\tTest: Loss: 0.7937, Accuracy: 57.9462%, Time: 158.5325s\n",
            "Best Accuracy for test : 0.5844 at epoch 012\n",
            "Epoch: 15/30\n",
            "Epoch: 015, Training: Loss: 0.5104, Accuracy: 75.0613%, \n",
            "\t\tTest: Loss: 0.9354, Accuracy: 51.5892%, Time: 158.0763s\n",
            "Best Accuracy for test : 0.5844 at epoch 012\n",
            "Epoch: 16/30\n",
            "Epoch: 016, Training: Loss: 0.4522, Accuracy: 78.9216%, \n",
            "\t\tTest: Loss: 0.8053, Accuracy: 57.7017%, Time: 158.7738s\n",
            "Best Accuracy for test : 0.5844 at epoch 012\n",
            "Epoch: 17/30\n",
            "Epoch: 017, Training: Loss: 0.4465, Accuracy: 79.6569%, \n",
            "\t\tTest: Loss: 0.8312, Accuracy: 58.6797%, Time: 158.5262s\n",
            "Best Accuracy for test : 0.5868 at epoch 017\n",
            "Epoch: 18/30\n",
            "Epoch: 018, Training: Loss: 0.4298, Accuracy: 80.3309%, \n",
            "\t\tTest: Loss: 0.8946, Accuracy: 59.1687%, Time: 158.9789s\n",
            "Best Accuracy for test : 0.5917 at epoch 018\n",
            "Epoch: 19/30\n",
            "Epoch: 019, Training: Loss: 0.4334, Accuracy: 79.4118%, \n",
            "\t\tTest: Loss: 0.8945, Accuracy: 56.9682%, Time: 158.1413s\n",
            "Best Accuracy for test : 0.5917 at epoch 018\n",
            "Epoch: 20/30\n",
            "Epoch: 020, Training: Loss: 0.3672, Accuracy: 83.2721%, \n",
            "\t\tTest: Loss: 0.9523, Accuracy: 57.7017%, Time: 158.7659s\n",
            "Best Accuracy for test : 0.5917 at epoch 018\n",
            "Epoch: 21/30\n",
            "Epoch: 021, Training: Loss: 0.3455, Accuracy: 84.9265%, \n",
            "\t\tTest: Loss: 1.0339, Accuracy: 55.9902%, Time: 158.8399s\n",
            "Best Accuracy for test : 0.5917 at epoch 018\n",
            "Epoch: 22/30\n",
            "Epoch: 022, Training: Loss: 0.3187, Accuracy: 86.4583%, \n",
            "\t\tTest: Loss: 1.0156, Accuracy: 54.5232%, Time: 158.9844s\n",
            "Best Accuracy for test : 0.5917 at epoch 018\n",
            "Epoch: 23/30\n",
            "Epoch: 023, Training: Loss: 0.2893, Accuracy: 88.1740%, \n",
            "\t\tTest: Loss: 0.9476, Accuracy: 58.1907%, Time: 159.1016s\n",
            "Best Accuracy for test : 0.5917 at epoch 018\n",
            "Epoch: 24/30\n",
            "Epoch: 024, Training: Loss: 0.2426, Accuracy: 90.1961%, \n",
            "\t\tTest: Loss: 0.9943, Accuracy: 58.9242%, Time: 159.5058s\n",
            "Best Accuracy for test : 0.5917 at epoch 018\n",
            "Epoch: 25/30\n",
            "Epoch: 025, Training: Loss: 0.2227, Accuracy: 91.6667%, \n",
            "\t\tTest: Loss: 1.1560, Accuracy: 56.7237%, Time: 159.1221s\n",
            "Best Accuracy for test : 0.5917 at epoch 018\n",
            "Epoch: 26/30\n",
            "Epoch: 026, Training: Loss: 0.2464, Accuracy: 89.7059%, \n",
            "\t\tTest: Loss: 1.0577, Accuracy: 59.1687%, Time: 159.3369s\n",
            "Best Accuracy for test : 0.5917 at epoch 018\n",
            "Epoch: 27/30\n",
            "Epoch: 027, Training: Loss: 0.2167, Accuracy: 91.2377%, \n",
            "\t\tTest: Loss: 1.0391, Accuracy: 56.2347%, Time: 158.9037s\n",
            "Best Accuracy for test : 0.5917 at epoch 018\n",
            "Epoch: 28/30\n",
            "Epoch: 028, Training: Loss: 0.2065, Accuracy: 91.7892%, \n",
            "\t\tTest: Loss: 1.0739, Accuracy: 57.4572%, Time: 159.0875s\n",
            "Best Accuracy for test : 0.5917 at epoch 018\n",
            "Epoch: 29/30\n",
            "Epoch: 029, Training: Loss: 0.1840, Accuracy: 93.4436%, \n",
            "\t\tTest: Loss: 1.0962, Accuracy: 56.4792%, Time: 158.4061s\n",
            "Best Accuracy for test : 0.5917 at epoch 018\n",
            "Epoch: 30/30\n",
            "Epoch: 030, Training: Loss: 0.1850, Accuracy: 92.7696%, \n",
            "\t\tTest: Loss: 1.0917, Accuracy: 55.9902%, Time: 158.9123s\n",
            "Best Accuracy for test : 0.5917 at epoch 018\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-4368586ff9b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m#make the plots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test Loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8P8atew7yG8"
      },
      "source": [
        "PATH = './real_fake.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}